{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "\n",
    "# !pip install llama-index\n",
    "# !pip install llama-index-core\n",
    "# !pip install llama-index-embeddings-openai\n",
    "# !pip install llama-parse\n",
    "# !pip install llama-index-vector-stores-kdbai\n",
    "# !pip install pandas\n",
    "# !pip install llama-index-postprocessor-cohere-rerank\n",
    "# !pip install kdbai_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.vector_stores.kdbai import KDBAIVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding   \n",
    "from llama_index.core import VectorStoreIndex  \n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.llms.openai import OpenAI     \n",
    "from llama_index.core import Settings\n",
    "from llama_parse import LlamaParse\n",
    "import kdbai_client as kdbai\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the async code in a notebook requires the use of nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "KDBAI_ENDPOINT = (\n",
    "    os.environ[\"KDBAI_ENDPOINT\"]\n",
    ")\n",
    "KDBAI_API_KEY = (\n",
    "    os.environ[\"KDBAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "#connect to KDB.AI\n",
    "session = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The schema contains two metadata columns (document_id, text) and one embeddings column\n",
    "# Index type, search metric (Euclidean distance), and dimensions are specified in the embedding column\n",
    "\n",
    "schema = dict(\n",
    "    columns=[\n",
    "        dict(name=\"document_id\", pytype=\"bytes\"),\n",
    "        dict(name=\"text\", pytype=\"bytes\"),\n",
    "        dict(\n",
    "            name=\"embedding\",\n",
    "            vectorIndex=dict(type=\"flat\", metric=\"L2\", dims=1536),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "KDBAI_TABLE_NAME = \"LlamaParse_Table\"\n",
    "\n",
    "# First ensure the table does not already exist\n",
    "if KDBAI_TABLE_NAME in session.list():\n",
    "    session.table(KDBAI_TABLE_NAME).drop()\n",
    "\n",
    "#Create the table\n",
    "table = session.create_table(KDBAI_TABLE_NAME, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL  = \"text-embedding-3-small\"\n",
    "GENERATION_MODEL = \"gpt-4o\"\n",
    "\n",
    "llm = OpenAI(model=GENERATION_MODEL)\n",
    "embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Parsing Instructions\n",
    "\n",
    "parsing_instructions = '''Answer questions using the information in this pdf and be precise. Avoid Hallucinations, and say you don't know if given data is not enough to answer the question'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Resume\"\n",
    "path = os.path.join('.', 'data', f'{name}.pdf')\n",
    "pickle_path = os.path.join('.', 'data', f'parsed_{name}_documents.pkl')\n",
    "\n",
    "if os.path.exists(pickle_path):\n",
    "    with open(pickle_path, 'rb') as file:\n",
    "        documents= pickle.load(file)\n",
    "        print(\"Loaded documents\")\n",
    "else:\n",
    "    documents = LlamaParse(result_type=\"markdown\", parsing_instructions=parsing_instructions).load_data(path)\n",
    "    with open(pickle_path, 'wb') as pickle_file:\n",
    "        pickle.dump(documents, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the documents using MarkdownElementNodeParser\n",
    "\n",
    "node_parser = MarkdownElementNodeParser(llm=llm, num_workers=8).from_defaults()\n",
    "\n",
    "# Retrieve nodes (text) and objects (table)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = KDBAIVectorStore(table)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "#Create the index, inserts base_nodes and objects into KDB.AI\n",
    "recursive_index = VectorStoreIndex(\n",
    "    nodes= base_nodes + objects, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# Query KDB.AI to ensure the nodes were inserted\n",
    "table.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define reranker\n",
    "cohere_rerank = CohereRerank(top_n=10)\n",
    "\n",
    "### Create the query_engine to execute RAG pipeline using LlamaIndex, KDB.AI, and Cohere reranker\n",
    "query_engine = recursive_index.as_query_engine(similarity_top_k=15, node_postprocessors=[cohere_rerank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = '''Who is Aditya Kushal'''\n",
    "\n",
    "response_1 = query_engine.query(query_1)\n",
    "\n",
    "print(str(response_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
